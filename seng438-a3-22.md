**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report #3 – Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group 22      |
| -------------- |
| Rhishik Roy |
| Kaniz Taiba |
| Ethan Reed |
| Michael Svoboda |
| Saim Shahzad |

(Note that some labs require individual reports while others require one report
for each group. Please see each lab document for details.)

# 1 Introduction

In this laboratory session, we delved into the fundamental concepts and application of white-box testing. Our exploration entailed the utilization and expansion of our pre-existing test suite, alongside the incorporation of novel coverage metrics such as statement and branch coverage. We crafted fresh white-box test cases aimed at augmenting code coverage and assessed the enhancements vis-à-vis our initial implementation. Moreover, a scrutiny of specific methods' data-flow coverage was conducted.

Through the utilization of white-box testing methodologies integrated with coverage tools, we accomplish the following objectives: Ensuring thorough testing of all code pathways including branches, loops, and exception handling. Identifying areas within the codebase that lack sufficient testing, thus enabling targeted improvements to the test suite. Improving the overall design and maintainability of the codebase. Detecting and rectifying errors at an early stage in the development lifecycle.

# 2 Manual data-flow coverage calculations

## DataUtilities.calculateColumnTotal

### Data flow graph and def-use sets

![calculateColumnTotalDataFlow](https://github.com/seng438-winter-2024/seng438-a3-rhishikr/assets/91651834/15e34331-344d-48e9-aa05-37020f59b176)

### Def-use pairs

| variable | du-pairs |
|----------|----------|
| data | (0,1), (0,3), (0,6), (0,12) |
| total | (2,8), (2,14), (2,16), (8,8), (8,14), (8,16), (14,14), (14,16) |
| rowCount | (3,5), (3,11) |
| r | (4,5), (4,6), (4,9), (9,5), (9,6) (9,9) |
| n (first loop) | (6,7), (6,8) |
| r2 | (10,11), (10,12), (10,15), (15,11), (15,12), (15,15) |
| n (second loop) | (12,13), (12,14) |

### Test case du-pair coverage

| test case | du-pairs |
|-----------|----------|
| calculateColumnTotalWithNoRows | data:{ (0,1), (0,3) }, total:{ (2,16) }, rowCount:{ (3,5), (3,11) }, r:{ (4,5) }, r2:{ (10,11) } |
| calculateColumnTotalSingleRow | data:{ (0,1), (0,3), (0,6) }, total:{ (2,8), (8,16) }, rowCount:{ (3,5), (3,11) }, r:{ (4,5), (4,6), (4,9), (9,5) }, n(l1):{ (6,7), (6,8) }, r2:{ (10,11) } |
| calculateColumnTotalMultiRowPositive | data:{ (0,1), (0,3), (0,6) }, total:{ (2,8), (8,8), (8,16) }, rowCount:{ (3,5), (3,11) },r:{ (4,5), (4,6), (4,9), (9,5), (9,6) (9,9) }, n(l1):{ (6,7), (6,8) }, r2:{ (10,11) } |
| calculateColumnTotalMultiRowNegative | data:{ (0,1), (0,3), (0,6) }, total:{ (2,8), (8,8), (8,16) }, rowCount:{ (3,5), (3,11) },r:{ (4,5), (4,6), (4,9), (9,5), (9,6) (9,9) }, n(l1):{ (6,7), (6,8) }, r2:{ (10,11) } |
| calculateColumnTotalMultiRowZero | data:{ (0,1), (0,3), (0,6) }, total:{ (2,8), (8,8), (8,16) }, rowCount:{ (3,5), (3,11) },r:{ (4,5), (4,6), (4,9), (9,5), (9,6) (9,9) },n(l1):{ (6,7), (6,8) }, r2:{ (10,11) } |
| calculateColumnTotalNullData | data:{ (0,1) } |
| calculateColumnTotalColumnOutOfRange | data:{ (0,1), (0,3), (0,6) }, total:{ (2,8), (8,16) }, rowCount:{ (3,5), (3,11) }, r:{ (4,5), (4,6), (4,9), (9,5) }, n(l1):{ (6,7), (6,8) }, r2:{ (10,11) } |
| calculateColumnTotalColumnNegative | data:{ (0,1), (0,3), (0,6) }, total:{ (2,8), (8,16) }, rowCount:{ (3,5), (3,11) }, r:{ (4,5), (4,6), (4,9), (9,5) }, n(l1):{ (6,7), (6,8) }, r2:{ (10,11) } |

du-pair coverage = 18/30 = 60%

## Range.combine

### Data flow graph and def-use sets

![combineDataFlow](https://github.com/seng438-winter-2024/seng438-a3-rhishikr/assets/91651834/72bb9c17-7845-447f-9de3-2e7bb95606bd)

### Def-use pairs

| variable | du-pairs |
|----------|----------|
| range1 | (0,1), (0,4), (0,5), (0,6) |
| range2 | (0,2), (0,3), (0,5), (0,6) |
| l | (5,7) |
| u | (6,7) |

### Test case du-pair coverage

| test case | du-pairs |
|-----------|----------|
| combineAllBoundsEqualLower | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineAllBoundsEqualUpper | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineIntersectingLower | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineIntersectingUpper | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineIntersectingNegativeLower | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineIntersectingNegativeUpper | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineNotIntersectingLower1 | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineNotIntersectingUpper1 | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineNotIntersectingLower2 | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineNotIntersectingUpper2 | range1:{ (0,1), (0,5), (0,6) }, range2:{ (0,3), (0,5), (0,6) }, l:{ (5,7) }, u:{ (6,7) } |
| combineNullWithRangeLower1 | range1:{ (0,1) }, range2:{ (0,2) } |
| combineNullWithRangeUpper1 | range1:{ (0,1) }, range2:{ (0,2) } |
| combineNullWithRangeLower2 | range1:{ (0,1), (0,4) }, range2:{ (0,3) } |
| combineNullWithRangeUpper2 | range1:{ (0,1), (0,4) }, range2:{ (0,3) } |
| combineNullWithNull | range1:{ (0,1) }, range2:{ (0,2) } |

du-pair coverage = 10/10 = 100%

# 3 A detailed description of the testing strategy for the new unit test

Our strategy involves reviewing the code implemented in our prior tests for Assignment 2, with the aim of identifying any gaps in our plans and test coverage. Through this process, we aim to gain insights into the metrics guiding our work and enhance the quality of these files by introducing additional tests. By regularly assessing our metrics and existing test coverage, we can pinpoint areas requiring new tests, taking into account both what has already been tested and what is currently under development. Our tool of choice for this endeavor is EmlEmma, which seamlessly integrates with Eclipse, our primary development platform.

# 4 A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage

Text…

# 5 A detailed report of the coverage achieved of each class and method (a screen shot from the code cover results in green and red color would suffice)

Initial coverage metrics:

![InitialCoverage](https://github.com/seng438-winter-2024/seng438-a3-rhishikr/assets/91651834/3a6c0a0f-7581-454e-b6cf-fc115b3961bf)


# 6 Pros and Cons of coverage tools used and Metrics you report

Pros of using EclEmma for test coverage:

-Integration with Eclipse IDE: EclEmma seamlessly integrates with Eclipse, making it easy to set up and use within the development environment.

-Multiple coverage metrics: EclEmma offers a variety of coverage metrics, including statement (line), branch, and method coverage, providing comprehensive insights into test coverage.

-Additional coverage measurements: Apart from the metrics chosen, EclEmma offers additional coverage measurement options, allowing for a more thorough analysis of code coverage.

-Color-highlight feedback: The tool provides color-highlight feedback on code, facilitating the identification of lines with errors or lacking coverage.

-Improvement guidance: EclEmma highlights covered lines of code for statement coverage, aiding in the improvement of test code to achieve better coverage.

Cons of using EclEmma for test coverage:

-Dark highlight colors: The highlight colors provided by EclEmma might be too dark, which can make it difficult to read the code, potentially causing strain on the eyes.

-Lack of condition coverage: EclEmma does not offer condition coverage, necessitating a switch to method coverage, which might not provide as detailed insights into code coverage.

-Limited information for branch coverage: EclEmma does not provide sufficient information to specify the number of states for branch coverage, making it challenging to improve branch coverage and debug effectively.

# 7 A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation.


---------Advantages of Requirements-driven Test Generation:

User-centric approach: Prioritizing user needs ensures that the software aligns with requirements, fostering the creation of comprehensive test cases covering all functional aspects.

Enhanced traceability: This method establishes a clear link between requirements and test cases, facilitating effective tracking of testing progress throughout the software development lifecycle.

Thorough requirement validation: By rigorously testing all requirements, this approach aids in identifying and addressing any gaps or discrepancies, thereby enhancing software reliability.

-----------Advantages of Coverage-oriented Test Generation:

Comprehensive code execution: Ensures thorough code coverage during testing, leading to faster identification of potential issues as it focuses primarily on code execution rather than requirements.

Scalability: This approach can be easily adapted and scaled to accommodate testing for large-scale software systems, ensuring efficient testing across various project sizes.

---------------Limitations of Requirements-driven Test Generation:

Neglect of non-functional requirements: Primarily focusing on functional requirements may result in inadequate testing of non-functional aspects such as performance and security, potentially leaving critical vulnerabilities unaddressed.

Test case generation overhead: The process of generating suitable test cases requires significant effort and resources, adding complexity to the testing process.

---------------Limitations of Coverage-based Test Generation:

Partial requirement coverage: While it tests the implemented code, it may overlook some functional requirements, potentially leading to gaps in testing coverage.

Misleading code coverage metrics: High code coverage does not guarantee thorough software testing, as it's possible to have defects despite comprehensive code coverage. Moreover, maintaining code-based tests requires continuous updates to reflect changes in the codebase.

# 8 A discussion on how the team work/effort was divided and managed

The workload was evenly distributed among all team members, fostering a collaborative environment conducive to efficient progress. Rhishik, Kaniz and Ethan undertook responsibilities pertaining to the DataUtilities class, while Michael and Saim focused on tasks related to the Range class. Upon completing their respective deliverables, the team engaged in thorough sharing sessions, highlighting key aspects of their work. Through this collective review process, potential errors were identified, and strategies for enhancing results and analysis were proposed. Our team, akin to previous assignments, played a pivotal role in ensuring the quality of our deliverables. Leveraging multiple perspectives proved instrumental in garnering suggestions, rectifying errors, exploring alternative solutions, and ultimately enhancing the overall lab experience. Furthermore, consistent communication via our group chat platform ensured alignment among team members, mitigating the risk of incomplete deliverables by the deadline.

# 9 Any difficulties encountered, challenges overcome, and lessons learned from performing the lab

Initially, comprehending Coverage and mastering the application of Eclemma posed a significant challenge. Our early efforts were concentrated on understanding these concepts and familiarizing ourselves with the associated tools and coverage methodologies. Once we had established a collective understanding, we proceeded to test and distinguish between boundary value analysis and equivalence class analysis. Despite encountering obstacles along the way, we successfully executed all tests within two groups through pair testing. This hands-on lab experience underscored the importance of discerning various forms of analysis in white box testing.

# 10 Comments/feedback on the lab itself

The lab experience proved profoundly impactful in offering students a practical immersion into software testing techniques. Exploring the diverse avenues of software testing proved both intriguing and enlightening. While initially confident in the adequacy of our methods post-Assignment 2, a brief delve into JUnit files with EclEmma revealed ample room for enhancement. The utilization of software coverage applications provided invaluable real-time insights into evolving coverage rates. In essence, the lab afforded a captivating glimpse into the professional testing landscape. Yet, amidst its merits, the assignment's setup could benefit from refinement, potentially streamlining processes to allocate more time for students to refine their testing proficiencies rather than grappling with debugging intricacies and library concerns.
